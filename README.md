# ğŸ“ˆ Time Series Analysis and Forecasting  

This repository provides a complete pipeline for **time-series analysis**, **pre-processing**, **feature engineering**, **model training**, and **forecasting**.

## âœ”ï¸ This Repository Covers  
1. **Introduction to Time Series**  
2. **Data Preparation**  
   - Normalization (0â€“1)  
   - Stationarity checking (ADF Test)  
3. **Lag Feature Selection**  
4. **Trainâ€“Test Splitting**  
5. **Regression, ML, and DL Models**  
   - Model training  
   - Prediction & performance evaluation  

â–¶ï¸ **To run the project:**  
Run **`Main.py`** â€” it executes the entire workflow.  
You may also run individual sections by commenting/uncommenting blocks.

Your only required input is your dataset:  
```python
data = your_data


---
 
```markdown
# 1ï¸âƒ£ What is a Time Series?  

A **time series** is a sequence of observations recorded over time (e.g., stock prices, temperature, heart rate).

### **Time Series Analysis**  
Used to extract **trends**, **seasonality**, **correlations**, and cycles before forecasting.

### **Time Series Forecasting**  
Building models that **predict future values** using historical patterns.

# 2ï¸âƒ£ Data Pre-processing  

Data preprocessing ensures that the dataset is clean and suitable for modeling.

### ğŸ”¹ Missing Values  
Handled using interpolation or pandas functions.

### ğŸ”¹ Normalization  
Used when features have different scales. Needed for:  
- KNN  
- Linear Regression  
- Neural Networks  
- Distance-based models  

### ğŸ”¹ Standardization  
Transforms data to zero mean & unit variance.

### ğŸ”¹ Stationarity Check  
A time series is stationary when:  
- Mean is constant  
- Variance is constant  
- Autocovariance is time-independent  

Check using:  
- Rolling mean/variance  
- **ADF test** (p < 0.05 â†’ stationary)

# 3ï¸âƒ£ Lag Features (Windowing)  

Lag features represent **previous time steps** used as predictors (lag-1, lag-2, lag-3, etc.).

### **Autocorrelation (ACF)**  
Shows correlation between values and their past values.

### **Partial Autocorrelation (PACF)**  
Shows correlation after removing the effects of earlier lags.  
Helps determine the **optimal lag p** for AR-based models.

# 4ï¸âƒ£ Trainâ€“Test Split  

- **Train set** â†’ used for model training  
- **Validation set** â†’ for tuning hyperparameters  
- **Test set** â†’ for final evaluation  

# 5ï¸âƒ£ Regression & Forecasting Models  

## ğŸ”· Linear Models  
- **Linear Regression (LR)**  
- **Least Squares Regression (LS)**  
- **Moving Average (MA)**  
- **Autoregressive (AR)**  
- **ARX Model**  
- **ARIMA (p, d, q)**  

Linear models:  
âœ”ï¸ Fast & interpretable  
âŒ Only linear  
âŒ Limited noise handling  
âŒ Not suited for multivariate series  
âŒ Focus on one-step forecasting

## ğŸ”· Machine Learning Models  
- **XGBoost Regression**  
- **Linear Regression**  
- **Decision Tree Regression**  
- **Random Forest Regression**

Workflow:  
1. Feed data + engineered features  
2. Train  
3. Test  
4. Predict future values  
  
## ğŸ”· Deep Learning Models (LSTM)  

LSTM is a powerful recurrent neural network used for sequence prediction.  
It is sensitive to scaling â†’ normalization is required.

### Types  
- **Vanilla LSTM**  
- **Stacked LSTM**  
- **Bidirectional LSTM**  
- **LSTM Autoencoder**  

### Keras Workflow  
1. Define model  
2. Compile  
3. Fit  
4. Evaluate  
5. Predict

# ğŸ“¦ Install Required Packages  

```bash
pip install numpy
pip install scipy
pip install pandas
pip install seaborn
pip install matplotlib
pip install scikit-learn
pip install keras
  

--- 
```markdown
# ğŸ§  Main Code Structure  
 
```python
data = sm.datasets.sunspots.load_pandas().data["SUNACTIVITY"]
data, normalize = normalize_data(data, Type_Normalize='MinMaxScaler', Display_Figure='on')
data = test_stationary(data, window=20)
auto_correlation(data, nLags=10)
nLags = 3
train_size = int(len(data) * 0.8)
train_x, train_y = sequences_data(np.array(data[:train_size]), nLags)
test_x, test_y = sequences_data(np.array(data[train_size:]), nLags)


